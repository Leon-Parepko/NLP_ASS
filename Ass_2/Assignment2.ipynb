{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:42.924992Z",
     "end_time": "2024-03-23T00:25:42.962798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "SPECIAL_TOKENS = {'<NUM>', '<UNK>'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:43.383528Z",
     "end_time": "2024-03-23T00:25:43.390895Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset for a training part is a hybrid of two datasets. The first one is a big.txt file from Norvig's solution, and the second one is a bigrams.txt file from the ngrams dataset. The ones can be downloaded manually from the following resources:\n",
    "\n",
    "- big.txt - https://norvig.com/big.txt\n",
    "- bigrams.txt - provided in the assignment on moodle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020385\n"
     ]
    },
    {
     "data": {
      "text/plain": "[(('of', 'the'), 2586813),\n (('in', 'the'), 2043262),\n (('to', 'the'), 1055301),\n (('on', 'the'), 920079),\n (('and', 'the'), 737714),\n (('to', 'be'), 657504),\n (('at', 'the'), 617976)]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ngrams from bigrams.txt which is structured as count\\tword1\\tword2\n",
    "N_GRAMS = Counter()\n",
    "with open('bigrams.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        count, word1, word2 = line.split()\n",
    "        N_GRAMS[(word1, word2)] = int(count)\n",
    "\n",
    "print(len(N_GRAMS))\n",
    "N_GRAMS.most_common(7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:44.303123Z",
     "end_time": "2024-03-23T00:25:46.187797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Calculate probabilities for each n-gram\n",
    "total_n_grams = sum(N_GRAMS.values())\n",
    "n_gram_probs = {n_gram: count / total_n_grams for n_gram, count in N_GRAMS.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:46.175905Z",
     "end_time": "2024-03-23T00:25:46.647698Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can drop the numbers and punctuation from the text because the task of spellchecking is to correct the words, not the numbers or punctuation. We can also lowercase the text to make it easier to work with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37429\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('the', 79178),\n ('of', 39995),\n ('and', 38092),\n ('to', 28610),\n ('in', 21754),\n ('a', 20843),\n ('he', 12182)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the big.txt dataset and preprocess it\n",
    "text = open('big.txt').read().lower()\n",
    "text = re.sub(r'\\d+', '<NUM>', text)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = re.findall(r'\\w+', text)\n",
    "WORDS = Counter(text)\n",
    "\n",
    "print(len(WORDS))\n",
    "WORDS.most_common(7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:46.653263Z",
     "end_time": "2024-03-23T00:25:47.412285Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us describe the main algorithm. So the idea is to mix the spellchecked word predictions of Norvig's solution with the predictions of the n-gram model. The algorithm tries to increase the accuracy and add the context to such Norvig's solution. The algorithm is as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def predict_word(word, n):\n",
    "    \"\"\"\n",
    "    Predict the first or the second word in bigram, given the other word\n",
    "    :param word: str - the word to predict from\n",
    "    :param n: int - the position of the word in the bigram,\n",
    "    0 - if predict first word given the second,\n",
    "    1 - if predict the second word given the first\n",
    "    :return: the tuple of the predicted words and its probabilities\n",
    "    \"\"\"\n",
    "    next_word_probs = {}\n",
    "\n",
    "    # Iterate over all n-grams and get the probabilities of the next word\n",
    "    for n_gram, prob in n_gram_probs.items():\n",
    "\n",
    "        # Add the probability of the predicted words\n",
    "        if n_gram[n-1] == word:\n",
    "            next_word_probs[n_gram[n]] = prob\n",
    "\n",
    "    return next_word_probs\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"\"\"\n",
    "    Probability of `word`.\n",
    "    :param word: str - the word to get the probability of\n",
    "    :param N: int - the total number of words in the dataset\n",
    "    :return: float - the probability of the word\n",
    "    \"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def check_correctness(word):\n",
    "    \"\"\"\n",
    "    Check if the word is correct or not according to the Norvig's solution\n",
    "    :param word: str - the word to check\n",
    "    :return: bool, tuple - the flag if the word is correct and the candidates for the word\n",
    "    \"\"\"\n",
    "\n",
    "    # If word is a special token, then it is correct\n",
    "    if word in SPECIAL_TOKENS:\n",
    "        return True, {}\n",
    "\n",
    "    # Get the candidates\n",
    "    candidates = get_candidates(word)\n",
    "\n",
    "    # if it is only one candidate and it is the same as the word, then it is correct\n",
    "    if len(candidates) == 1 and word in candidates:\n",
    "        return True, candidates\n",
    "\n",
    "    # if the word is in the dictionary, then it is incorrect\n",
    "    else:\n",
    "        return False, candidates\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    \"\"\"\n",
    "    Get the most probable correction of the word\n",
    "    :param word: str - the word to correct\n",
    "    :return: str - the corrected word\n",
    "    \"\"\"\n",
    "    return max(get_candidates(word), key=P)\n",
    "\n",
    "\n",
    "def get_candidates(word):\n",
    "    \"\"\"\n",
    "    Get the candidates for the word\n",
    "    :param word: str - the word to get the candidates for\n",
    "    :return: list - the list of candidates\n",
    "    \"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"\n",
    "    Get the words that are in the dictionary\n",
    "    :param words: list - the list of words to check\n",
    "    :return: set - the set of words that are in the dictionary\n",
    "    \"\"\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Get all edits that are one edit away from `word`.\n",
    "    :param word: str - the word to get the edits for\n",
    "    :return: set - the set of edits\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"\n",
    "    Get all edits that are two edits away from `word`.\n",
    "    :param word: str - the word to get the edits for\n",
    "    :return: set - the set of edits\n",
    "    \"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "def fix_sentence(sentence):\n",
    "    \"\"\"\n",
    "    The main algorithm to fix the sentence\n",
    "    :param sentence: str - the sentence to fix\n",
    "    :return: str - the fixed sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the sentence as were done before\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    sentence = re.sub(r'\\d+', '<NUM>', sentence)\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Iterate over all words\n",
    "    for i, word in enumerate(words):\n",
    "        check, candidates = check_correctness(word)\n",
    "\n",
    "        # If the word is incorrect, then fix it\n",
    "        if not check:\n",
    "\n",
    "            # Get the Norvig's probabilities and couple them with candidates\n",
    "            norv_probs = {}\n",
    "            for c in candidates:\n",
    "                norv_probs[c] = P(c)\n",
    "            norv_probs = dict(sorted(norv_probs.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "            # Get two bigrams before and after the word if possible and make predictions\n",
    "            ngram_probs = {}\n",
    "            if i >= 1 and i <= len(words) - 2:\n",
    "                ngram_probs = {**predict_word(words[i-1], 1), **predict_word(words[i+1], 0)}\n",
    "\n",
    "            elif i == 0:\n",
    "                ngram_probs = predict_word(words[i+1], 0)\n",
    "\n",
    "            elif i == len(words) - 1:\n",
    "                ngram_probs = predict_word(words[i-1], 1)\n",
    "\n",
    "\n",
    "\n",
    "            # Normalize the probabilities\n",
    "            total_norv = sum(norv_probs.values())\n",
    "            total_ngram = sum(ngram_probs.values())\n",
    "            norv_probs = {k: v / total_norv for k, v in norv_probs.items()}\n",
    "            ngram_probs = {k: v / total_ngram for k, v in ngram_probs.items()}\n",
    "\n",
    "            # Ð¡ombine the prediction words with the candidates and if the key is the same, then take an average of this two\n",
    "            prediction_set = norv_probs\n",
    "            for p in ngram_probs:\n",
    "                if p in norv_probs:\n",
    "                    prediction_set[p] = (prediction_set[p] + ngram_probs[p]) / 2\n",
    "                else:\n",
    "                    prediction_set[p] = ngram_probs[p]\n",
    "\n",
    "            # Get the most probable word and fix the sentence\n",
    "            word_pred = max(prediction_set, key=prediction_set.get)\n",
    "            words[i] = word_pred\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:47.420723Z",
     "end_time": "2024-03-23T00:25:47.476610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'because i like the <NUM> is honored on the theres'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_sentence(\"Becouse, i likke the 1 is honoroded oon the theees\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:49.321931Z",
     "end_time": "2024-03-23T00:25:50.259924Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Justifying the decisions\n",
    "\n",
    "The Norvig's solution is a good spellchecker, but it does not take into account the context of the words. The n-gram model, on the other hand, is a good model to predict the next word given the previous one. So the idea is to combine these two models to increase the accuracy of the spellchecker.\n",
    "\n",
    "To simplify the algorithm, the Norvig's solution is used to detect words need to be corrected. So if the one did not offer the candidates at all or offer single one similar to check word, then the word treated as 'incorrect'. The approach is used because the methodology of the one is approved and well-known. Moreover, in case we want to improve the accuracy of Norvig's solution, such tagging ('correct', 'incorrect') is obligatory for the n-gram to further deal only with context dependent cases.\n",
    "\n",
    "The data used for the training part is a hybrid of two datasets. The first one is a big.txt file from Norvig's solution. I have chosen this dataset because it is a very good balanced corpus of English text. During the experiments with much bigger datasets ([wikipedia](https://www.kaggle.com/datasets/jkkphys/english-wikipedia-articles-20170820-sqlite), [english books](https://www.kaggle.com/datasets/raynardj/classic-english-literature-corpus), etc.) it was found that the word dictionary seems to be hard to filter and containing a lot of words with 'spell' mistakes.\n",
    "\n",
    "As for the N-gram dataset, I have chosen the bigrams.txt file from the ngrams dataset. The experiments show that the bigrams are the most suitable for the task of spellchecking because. Fivegrams and trigrams results in a high number of variations but with low possibility of being fitted to the context. In such cases, the n-gram model simply did not give any predictions, since that bigrams are used.\n",
    "\n",
    "Another important thing is the probability aggregation. In my model, the N-gram and Norvig's solutions are equally weighted. So to get the final prediction, the probabilities of the two models are normalized and averaged if both predicts the same word with different probability. Other words added to the total prediction set by union of the two models predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "For the evaluation part, it is used the [wikipedia-sentences](https://www.kaggle.com/datasets/mikeortman/wikipedia-sentences) dataset from kaggle. To make the evaluation more efficient, only the first 1000 sentences are used. The sentences are preprocessed by removing the punctuation, numbers, and lowercasing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": "['imanol sarriegi isasa born <NUM> april <NUM> is a spanish footballer who plays as a central midfielder',\n 'indeed the greater part of this chisian daniel cannot be said to deserve the name of a translation at all',\n 'after moving to los angeles to become an actor rambo started working in the real estate business',\n 'the rolladenschneider ls <NUM> is a standard class single seat glider manufactured by rolladenschneider flugzeugbau gmbh between <NUM> and <NUM> ']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('wikisent2.txt', 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# Shuffle the data\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "# Get only first 1000 sentences\n",
    "data = data[:1000]\n",
    "\n",
    "# Lowercase, remove punctuation, /n, and replace numbers with <NUM>\n",
    "data = [re.sub(r'[^\\w\\s]', '', line.lower()) for line in data]\n",
    "data = [re.sub(r'\\d+', ' <NUM> ', line) for line in data]\n",
    "data = [line.replace('\\n', '') for line in data]\n",
    "data = [re.sub(' +', ' ', line) for line in data]\n",
    "\n",
    "print(len(data))\n",
    "data[:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:25:54.814550Z",
     "end_time": "2024-03-23T00:26:05.433075Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To add the noise to the data I will simply modify each word in the sentence with a certain probability. So the word can be modified with 3 different ways:\n",
    "- Change a random letter\n",
    "- Add a random letter\n",
    "- Remove a random letter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 54074.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['imanol sarriegi isasa born <NUM> april <NUM> is m spanish jotballer who plays as a central midfielder',\n 'indeed the greater part of this chisian daniel cannot be said to deserve the nae of a translation at all',\n 'after moxing to los anheles tm become an actor rambo started working in the real estate business',\n 'the rolladenschneider l <NUM> j a standard class single reat glider manufactured by rolladenschneider flugzeugbau gmbh between <NUM> and <NUM>']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mistake_probability = 0.3          # Probability of a single word to be incorrect\n",
    "letter_change_prob = 0.4                # Probability of a letter to be randomly changed\n",
    "additional_letter_probability = 0.2     # Probability of a letter to be randomly added\n",
    "missing_letter_probability = 0.2        # Probability of a letter to be randomly removed\n",
    "\n",
    "\n",
    "def make_mistake(word):\n",
    "    \"\"\"\n",
    "    The function to make a mistake in the word\n",
    "    :param word: str - the word to make a mistake in\n",
    "    :return: str - the word with a mistakes (or not)\n",
    "    \"\"\"\n",
    "\n",
    "    # If the word is a special token, then do not make a mistake\n",
    "    if random.random() < word_mistake_probability and word not in SPECIAL_TOKENS:\n",
    "        if random.random() < letter_change_prob:\n",
    "            # Change a random letter\n",
    "            word = list(word)\n",
    "            word[random.randint(0, len(word) - 1)] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            word = \"\".join(word)\n",
    "\n",
    "        if random.random() < additional_letter_probability:\n",
    "            # Add a random letter\n",
    "            word = list(word)\n",
    "            word.insert(random.randint(0, len(word)), random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "            word = \"\".join(word)\n",
    "\n",
    "        if random.random() < missing_letter_probability:\n",
    "            # Remove a random letter\n",
    "            word = list(word)\n",
    "            word.pop(random.randint(0, len(word) - 1))\n",
    "            word = \"\".join(word)\n",
    "\n",
    "    return word\n",
    "\n",
    "# Make mistakes in the dataset\n",
    "data_mistakes = []\n",
    "for line in tqdm(data):\n",
    "    words = line.split()\n",
    "    words = [make_mistake(word) for word in words]\n",
    "    data_mistakes.append(\" \".join(words))\n",
    "\n",
    "data_mistakes[:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:27:10.443709Z",
     "end_time": "2024-03-23T00:27:10.487212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [12:32<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fix the sentences via my solution\n",
    "fix_sentences = []\n",
    "for line in tqdm(data_mistakes):\n",
    "    fix_sentences.append(fix_sentence(line))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:28:16.535008Z",
     "end_time": "2024-03-23T00:40:48.597845Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluation metric is the accuracy. To make it more precise the one is done via words (not sentences). So the function to get the similarity of two sentences is implemented."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_similarity(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    Get the similarity of two sentences via accuracy of guessing words.\n",
    "    :param sentence1: str - the first sentence\n",
    "    :param sentence2: str - the second sentence\n",
    "    :return: float - the accuracy of sentence guessing\n",
    "    \"\"\"\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(words1)):\n",
    "        if words1[i] == words2[i]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:40:51.748973Z",
     "end_time": "2024-03-23T00:40:51.806725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7636947190242973"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc = 0\n",
    "for i, sentence in enumerate(data_mistakes):\n",
    "    avg_acc += get_similarity(sentence, fix_sentences[i])\n",
    "\n",
    "avg_acc = avg_acc / len(data_mistakes)\n",
    "avg_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:40:53.593814Z",
     "end_time": "2024-03-23T00:40:53.634366Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:04<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7334544590272988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix and evaluate the sentences via my Norvig's solution\n",
    "avg_acc = 0\n",
    "for i, sentence in enumerate(tqdm(data_mistakes)):\n",
    "    avg_acc += get_similarity(sentence, \" \".join([correction(word) for word in data_mistakes[i].split()]))\n",
    "\n",
    "avg_acc = avg_acc / len(data_mistakes)\n",
    "avg_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T00:42:25.951035Z",
     "end_time": "2024-03-23T00:45:30.696529Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The difference between the accuracy of my solution and Norvig's one is not so significant but seems to be stable. The accuracy of my solution is a bit greater, probably because it captures the context based dependencies or simply because it increases the possible prediction candidates for the word.\n",
    "\n",
    "However, important to note that the time efficiency of my algorithm is much worse than Norvig's one. So the one need to be optimized.\n",
    "\n",
    "Finally, mistake probabilities for the evaluation dataset are chosen relatively high. This is done to make the evaluation more challenging and probably create the situations where the context dependent cases occure more often. In case of lower mistake probabilities, the accuracy of the Norvig's solution and my solution will be much closer."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
